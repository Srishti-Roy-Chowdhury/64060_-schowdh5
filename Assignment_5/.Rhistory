library(readr)
library(dplyr)
library(class)
library(gmodels)
library(tidyverse)
library(factoextra)
library(flexclust)
library(stats)
library(cluster)
library(caret)
cereals <- read_csv("./Cereals.csv")
head(cereals)       #77 observations
cereals <- read_csv("./Cereals.csv")
summary(cereals)       #77 observations
cereals <- na.omit(cereals) #removing the observation with null values
summary(cereals)            #3 observations have been removed, now the total is 74
#from the summary, it is clear that the value of sodium, potassium will influence the model due to its high magnitude nature, if not normalized
cereals.num <- cereals[, 4:16]  #creating dataset only with numeric variables
#Applying hierarchical clustering to the data using Euclidean distance to the normalized measurements.
df <- scale(cereals.num)   #normalizing data
dist <- dist(df, method="euclidean")   #calculating euclidean distance matrix
hc.ward <- hclust(dist, method="ward")
plot(hc.ward, cex=0.6, hang=-1)
#Agnes clustering with different linkage methods, using the normalized data
hc_single <- agnes(df, method="single")
hc_complete <- agnes(df, method="complete")
hc_average <- agnes(df, method="average")
hc_ward <- cluster::agnes(df, method="ward")
#comparing agglomerative coefficients
print(hc_single$ac)
print(hc_complete$ac)
print(hc_average$ac)
print(hc_ward$ac)
#Even if I use the euclidean distance data with agnes instead of the only normalized data, it shows the same result.
hc_ward1 <- agnes(dist, method="ward")
print(hc_ward1$ac)
#From agglomerative coefficient values, ward linkage is the best method for this dataset, as it shows the highest value, which is 0.9046042, among all four methods.
#From the lecture slides, we leanrned that agnes ( ) and hclust ( ) functions behave very similarly, so I am using ward linkage here to find the cluster number based on euclidean distance matrix
hc.ward <- hclust(dist, method="ward")
plot(hc.ward, cex=0.6, hang=-1)
#visualization of clusters
rect.hclust(hc.ward,k=4,border=1:3)
#adding the cluster number with the original dataset
df <- as.data.frame(df)
df.clusters <- cutree(hc.ward, k = 4)
df$cluster <- df.clusters
table(df.clusters)
#From the tree structure, 3 or 4 clusters would be a good separation of data. If cut the tree into 5 or more clusters then the chances of getting small groups are high. Also, then I will need to jump a larger vertical gaps. Finally, between those choice, I will proceed with cluster 4.
set.seed(246)
# Step 1: Partitioning the data (e.g., 70% training, 30% test)
train.index <- createDataPartition(1:nrow(df), p = 0.7, list = FALSE)
train.data=df[train.index,]     #70%
test.data=df[-train.index,]    #30%
#clustering partition A (training data)
train.data.dist <- dist(train.data, method="euclidean")
hc.clusterA <- hclust(train.data.dist, method="ward")  #making cluster partition A using Ward linkage
#doing this part for my visualization
plot(hc.clusterA, cex=0.6, hang=-1)
rect.hclust(hc.clusterA,k=4,border=1:4)
#calculating training data cluster centroids
train_clusters <- cutree(hc.clusterA, k = 4)
train.data <- as.data.frame(train.data)
train.data$cluster <- train_clusters
centroid.A <- aggregate(train.data, by = list(Cluster = train_clusters), mean)
#Assigning each record in B (test data) to nearest centroid
assign_to_cluster <- function(obs, centers) {
distances <- apply(centers, 1, function(c) sqrt(sum((obs - c)^2)))
which.min(distances)
}
test_prediction <- apply(test.data, 1, assign_to_cluster, centers = centroid.A)
#getting actual clusters of test dataset
fulldata.cluster <- hclust(dist, method="ward")
clusters <- cutree(as.hclust(fulldata.cluster), k = 4)
test_clusters <- clusters[-train.index]
#Assess stability using Adjusted Rand Index (ARI)
ari <- randIndex(test_prediction, test_clusters)
cat("Adjusted Rand Index (stability measure):", ari, "\n")
#The ARI value is 0.4808743 which represents somewhat stable relation among clusters
#To make the cluster of healthy diets, I am using the hierarchical clustering and ward linkage method. The code will be almost same as task-1
diet <- cereals
diet <- na.omit(diet) #removing the observation with null values
summary(diet)       #from the summary, it is clear that the value of potassium will influence the model due to its high magnitude nature, if not normalized
diet.num <- diet[, 4:16] #removing the non-numeric variables
diet <- scale(diet.num)   #normalizing data
summary(diet)           # now all variables are in same scale
dist <- dist(diet, method="euclidean")   #calculating euclidean distance matrix
#performing hierarchical clustering with ward linkage as it's agglomerative coefficient value is high (got from task-1)
hc.ward <- hclust(dist, method="ward")
plot(hc.ward, cex=0.6, hang=-1)
rect.hclust(hc.ward,k=4,border=1:3)
#adding the cluster number with the the scaled dataset
diet <- as.data.frame(df)
diet.clusters <- cutree(hc.ward, k = 4)
diet$cluster <- diet.clusters
#some cluster summary
table(df.clusters)
cereal.diet <- aggregate(diet, by = list(diet$cluster), FUN = mean)
#I will use the protein, fiber, vitamins, fat and potassium variables to interprate the clusters to select the best set of cereals
cereal.diet %>% group_by(cluster) %>% select(calories, protein, fiber, vitamins, fat, potass, sodium, sugars, carbo)
#The following conclusions were obtained based on the averages of each cluster:
#Cluster 1 = High Protein, High Fiber, High Potassium
#Cluster 2 = High Calories, High Protein, High Fiber,High Fat, High Potassium, High Sugars
#Cluster 3 = High Calories, No Fat, High Potassium, High Carbohydrates
#Cluster 4 = High Protein, High Vitamins, High Sodium, High Carbohydrates
#The choice of health cereals will depend on what nutrition we want to provide the children. For example, without no doubt high protein, high vitamins, high fiber cereals are good for chilbren, at the same time carbohydrate, calories, sodium, fat are good for children too but to a certain extent, for which I need expert help.
#Finally, I am in support of normalizing datasets. Otherwise Variables with larger numerical ranges (e.g., sodium, potassium) would influence the distance calculations.
